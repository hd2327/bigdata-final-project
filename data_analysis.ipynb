{
  "metadata": {
    "name": "final-project-test",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom operator import add\nfrom csv import reader\nfrom pyspark import SparkContext\n\nparking_v \u003d sc.textFile(\"/user/hd2327/data_cleaning_output.csv\")\n\nparking_v \u003d parking_v.mapPartitions(lambda x: reader(x))\n\nindex \u003d 2\n\nparking_v \u003d parking_v.filter(lambda x: len(x) \u003e 1) \\\n    .filter(lambda x: x[index] !\u003d \"BOROUGH\")\\\n    .filter(lambda x: x[index] !\u003d \"\")\\\n    .map(lambda x: (x[index], 1)) \\\n    .reduceByKey(add) \\\n    .sortBy(lambda x: x[1], False)\n\nresult \u003d parking_v\\\n        .map(lambda x: x[0] + \u0027\\t\u0027 + str(x[1]))\nresult.collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom operator import add\nfrom csv import reader\nfrom pyspark import SparkContext\n\nparking_v \u003d sc.textFile(\"/user/hd2327/data_cleaning_output.csv\")\n\nparking_v \u003d parking_v.mapPartitions(lambda x: reader(x))\n\nindex \u003d 21\n\nparking_v \u003d parking_v.filter(lambda x: len(x) \u003e 1) \\\n    .filter(lambda x: x[index] !\u003d \"VEHICLE TYPE CODE 1\")\\\n    .filter(lambda x: x[index] !\u003d \"\")\\\n    .map(lambda x: (x[index], 1)) \\\n    .reduceByKey(add) \\\n    .sortBy(lambda x: x[1], False)\n\nresult \u003d parking_v \\\n        .map(lambda x: x[0] + \u0027\\t\u0027 + str(x[1]))\nresult.collect()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom operator import add\nfrom csv import reader\nfrom pyspark import SparkContext\n\nparking_v \u003d sc.textFile(\"/user/hd2327/data_cleaning_output.csv\")\n\nparking_v \u003d parking_v.mapPartitions(lambda x: reader(x))\n\nindex \u003d 18\n\nparking_v \u003d parking_v.filter(lambda x: len(x) \u003e 1) \\\n    .filter(lambda x: x[index] !\u003d \"CONTRIBUTING FACTOR VEHICLE 1\")\\\n    .filter(lambda x: x[index] !\u003d \"\")\\\n    .map(lambda x: (x[index], 1)) \\\n    .reduceByKey(add) \\\n    .sortBy(lambda x: x[1], False)\n\nresult \u003d parking_v \\\n        .map(lambda x: x[0] + \u0027\\t\u0027 + str(x[1]))\nresult.collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom operator import add\nfrom csv import reader\nfrom pyspark import SparkContext\n\nparking_v \u003d sc.textFile(\"/user/hd2327/data_cleaning_output.csv\")\n\nparking_v \u003d parking_v.mapPartitions(lambda x: reader(x))\n\nindex \u003d 0\n\nparking_v \u003d parking_v.filter(lambda x: len(x) \u003e 1) \\\n    .filter(lambda x: x[index] !\u003d \"CRASH DATE\")\\\n    .filter(lambda x: x[index] !\u003d \"\")\\\n    .map(lambda x: (x[index], 1)) \\\n    .reduceByKey(add) \\\n    .sortBy(lambda x: x[1], False)\n\nresult \u003d sc.parallelize(parking_v.take(10)).map(lambda x: x[0] + \u0027\\t\u0027 + str(x[1]))\nresult.collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom operator import add\nfrom csv import reader\nfrom pyspark import SparkContext\n\nparking_v \u003d sc.textFile(\"/user/hd2327/data_cleaning_output.csv\")\n\nparking_v \u003d parking_v.mapPartitions(lambda x: reader(x))\n\nindex \u003d 1\n\nparking_v \u003d parking_v.filter(lambda x: len(x) \u003e 1) \\\n    .filter(lambda x: x[index] !\u003d \"CRASH TIME\")\\\n    .filter(lambda x: x[index] !\u003d \"\")\\\n    .map(lambda x: (x[index], 1)) \\\n    .reduceByKey(add) \\\n    .sortBy(lambda x: x[1], False)\n\nresult \u003d sc.parallelize(parking_v.take(10)).map(lambda x: x[0] + \u0027\\t\u0027 + str(x[1]))\nresult.collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom operator import add\nfrom csv import reader\nfrom pyspark import SparkContext\n\nparking_v \u003d sc.textFile(\"/user/hd2327/data_cleaning_output.csv\")\n\nparking_v \u003d parking_v.mapPartitions(lambda x: reader(x))\n\nindex \u003d 0\n\ndef state(x):\n    list \u003d x.split(\"/\")\n    return (list[0]+\u0027 month\u0027, 1)\n\n\nparking_v \u003d parking_v.filter(lambda x: len(x) \u003e 1) \\\n    .filter(lambda x: x[index] !\u003d \"CRASH DATE\") \\\n    .map(lambda x: (x[index]))\nparking_v \u003d parking_v.map(lambda x: state(x))\nparking_v \u003d parking_v.reduceByKey(add)\n\nresult \u003d parking_v.sortByKey() \\\n    .map(lambda x: x[0] + \u0027\\t\u0027 + str(x[1]))\n    \nresult.collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom operator import add\nfrom csv import reader\nfrom pyspark import SparkContext\n\nparking_v \u003d sc.textFile(\"/user/hd2327/data_cleaning_output.csv\")\n\nparking_v \u003d parking_v.mapPartitions(lambda x: reader(x))\n\nindex \u003d 1\n\ndef state(x):\n    list \u003d x.split(\":\")\n    time \u003d list[0]\n    if time in [\u00276\u0027, \u00277\u0027, \u00278\u0027, \u00279\u0027]:\n        return (\u00276-9\u0027, 1)\n    elif time in [\u002710\u0027, \u002711\u0027, \u002712\u0027, \u002713\u0027]:\n        return (\u002710-13\u0027, 1)\n    elif time in [\u002714\u0027, \u002715\u0027, \u002716\u0027, \u002717\u0027]:\n        return (\u002714-17\u0027, 1)\n    elif time in [\u002718\u0027, \u002719\u0027, \u002720\u0027, \u002721\u0027]:\n        return (\u002718-21\u0027, 1)\n    elif time in [\u002722\u0027, \u002723\u0027, \u00270\u0027, \u00271\u0027]:\n        return (\u002722-1\u0027, 1)\n    elif time in [\u00272\u0027, \u00273\u0027, \u00274\u0027, \u00275\u0027]:\n        return (\u00272-5\u0027, 1)\n\n\nparking_v \u003d parking_v.filter(lambda x: len(x) \u003e 1) \\\n    .filter(lambda x: x[index] !\u003d \"CRASH TIME\") \\\n    .map(lambda x: (x[index]))\nparking_v \u003d parking_v.map(lambda x: state(x))\nparking_v \u003d parking_v.reduceByKey(add)\\\n                .sortBy(lambda x: x[1], False)\n\nresult \u003d parking_v\\\n    .map(lambda x: x[0] + \u0027\\t\u0027 + str(x[1]))\n    \nresult.collect()\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}